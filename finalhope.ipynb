{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5szMPSaNfPKu",
        "HgOOVAHljzjv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/themukil/SummerInternProject/blob/main/finalhope.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYslCxpXWUcS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To import necessary modules and libraries for the TensorFlow and scikit-learn code"
      ],
      "metadata": {
        "id": "8z1cDlARXbXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "gax-J_cHWY2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Authenticating the user using the `google.colab` library in Python"
      ],
      "metadata": {
        "id": "Kio_pT1nX6t3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FkL1L81LWcIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting user's Google Drive using the `google.colab` library in Python"
      ],
      "metadata": {
        "id": "28hdvPO5YLnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/bq-results-20230716-111421-1689506072370/hope.csv')\n",
        "data = shuffle(data, random_state=22)\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "FTFVJoeVWfgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To read a CSV file from the Google Drive, shuffle the data, and display the first few rows of the dataset"
      ],
      "metadata": {
        "id": "Tt_AoVGzYjrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [label for label in data['label'].values]\n",
        "print(labels, '\\n')"
      ],
      "metadata": {
        "id": "3VefoaelWhmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To extract and print the 'label' values from a DataFrame"
      ],
      "metadata": {
        "id": "ysnHNG3-YeNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_labels = []\n",
        "for label in labels:\n",
        "  if label == 1:\n",
        "    text_labels.append('non-hope')\n",
        "  elif label == 0:\n",
        "    text_labels.append('hope')\n",
        "print(text_labels)\n",
        "txt_labels = [[element] for element in text_labels]\n",
        "print(txt_labels)"
      ],
      "metadata": {
        "id": "Bm7kJI_QWlAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To convert numerical labels(0, 1) into text labels(hope, non-hope) and then creating a list of lists containing the text labels"
      ],
      "metadata": {
        "id": "ZZAkqPhyZa_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size= int(len(data) * .8)\n",
        "print(f\"Train size: {train_size}\")\n",
        "print(f\"Test size: {len(data)-train_size}\")"
      ],
      "metadata": {
        "id": "N7fjiCVhWtCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate and print the train and test sizes based on a given dataset. 20% of the data from the dataset is used to train the model\n"
      ],
      "metadata": {
        "id": "dvvGDlPcZytM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = labels_encoded[:train_size]\n",
        "test_labels = labels_encoded[train_size:]"
      ],
      "metadata": {
        "id": "Y4EOybCaWv0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To split encoded labels into train and test sets"
      ],
      "metadata": {
        "id": "hAYmW0rZaM7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile preprocess.py\n",
        "\n",
        "from tensorflow.keras.preprocessing import text\n",
        "\n",
        "class TextPreprocessor(object):\n",
        "  def __init__(self, vocab_size):\n",
        "    self._vocab_size = vocab_size\n",
        "    self._tokenizer = None\n",
        "\n",
        "  def create_tokenizer(self, text_list):\n",
        "    tokenizer = text.Tokenizer(num_words=self._vocab_size)\n",
        "    tokenizer.fit_on_texts(text_list)\n",
        "    self._tokenizer = tokenizer\n",
        "\n",
        "  def transform_text(self, text_list):\n",
        "    text_matrix = self._tokenizer.texts_to_matrix(text_list)\n",
        "    return text_matrix"
      ],
      "metadata": {
        "id": "qiWZepuNW0yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To write the given code snippet into a file named `preprocess.py`"
      ],
      "metadata": {
        "id": "NW6RlXM_a6z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocess import TextPreprocessor\n",
        "\n",
        "VOCAB_SIZE=400# This is a hyperparameter, try out different values for your dataset\n",
        "\n",
        "train_qs = data['text'].values[:train_size]\n",
        "test_qs = data['text'].values[train_size:]\n",
        "\n",
        "processor = TextPreprocessor()\n",
        "processor._vocab_size=400\n",
        "processor.create_tokenizer(train_qs)\n",
        "\n",
        "body_train = processor.transform_text(train_qs)\n",
        "body_test = processor.transform_text(test_qs)"
      ],
      "metadata": {
        "id": "2XGeVPoeW17H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the `TextPreprocessor` class from the `preprocess.py` file and preprocess text data using it (VOCAB_SIZE is set to 400)"
      ],
      "metadata": {
        "id": "_msE5BsUbyIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(body_train[0]))\n",
        "print(body_train[0])"
      ],
      "metadata": {
        "id": "jenJ4gTWW65u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To print the length and the content of the processed text data in `body_train`"
      ],
      "metadata": {
        "id": "nhUVSRKjcEdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('./processor_state.pkl', 'wb') as f:\n",
        "  pickle.dump(processor, f)"
      ],
      "metadata": {
        "id": "CswkmThmW89v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save the `processor` object using the `pickle` module"
      ],
      "metadata": {
        "id": "WntylAyTcPOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(vocab_size, num_tags):\n",
        "\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(50, input_shape=(VOCAB_SIZE,), activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(25, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(num_labels, activation='sigmoid'))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "jTEVGpsjW9oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a model using the `tf.keras` library with specified layers and activation functions\n"
      ],
      "metadata": {
        "id": "Xzi6RC1XcdKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(VOCAB_SIZE, num_labels)\n",
        "model.summary()\n",
        "\n",
        "# Train and evaluate the model\n",
        "model.fit(body_train, train_labels, epochs=10, batch_size=128, validation_split=0.1)\n",
        "print('Eval loss/accuracy:{}'.format(\n",
        "  model.evaluate(body_test, test_labels, batch_size=128)))\n",
        "\n",
        "# Export the model to a file\n",
        "model.save('keras_saved_model.h5')"
      ],
      "metadata": {
        "id": "mZP1cLy3XBBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create, train, evaluate, and save a model as 'keras_saved_model.h5' using the `tf.keras` library (epochs = 10, batch_size = 128 and validation_split = 0.1)"
      ],
      "metadata": {
        "id": "RRKqCy4acZDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(body_test, test_labels, batch_size=128)"
      ],
      "metadata": {
        "id": "zqTiHiUQXEHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the model on the test data and print the evaluation metrics"
      ],
      "metadata": {
        "id": "8YEgvDrYdaMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_prediction.py\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class CustomModelPrediction(object):\n",
        "\n",
        "  def __init__(self, model, processor):\n",
        "    self._model = model\n",
        "    self._processor = processor\n",
        "\n",
        "  def predict(self, instances, **kwargs):\n",
        "    preprocessed_data = self._processor.transform_text(instances)\n",
        "    predictions = self._model.predict(preprocessed_data)\n",
        "    return predictions.tolist()\n",
        "\n",
        "  @classmethod\n",
        "  def from_path(cls, model_dir):\n",
        "    import tensorflow.keras as keras\n",
        "    model = keras.models.load_model(\n",
        "      os.path.join(model_dir,'keras_saved_model.h5'))\n",
        "    with open(os.path.join(model_dir, 'processor_state.pkl'), 'rb') as f:\n",
        "      processor = pickle.load(f)\n",
        "\n",
        "    return cls(model, processor)\n"
      ],
      "metadata": {
        "id": "K-n1Lx4WXJAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_requests = [\"I hope everyone's life is blessed with happiness, joy and prosperity\", \"The world is a horrible place and everyone must die\"]"
      ],
      "metadata": {
        "id": "mkFf1Ol4XLqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Providing input to the model"
      ],
      "metadata": {
        "id": "AvqX7-Otdlfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from model_prediction import CustomModelPrediction\n",
        "\n",
        "classifier = CustomModelPrediction.from_path('.')\n",
        "results = classifier.predict(test_requests)\n",
        "print(results)\n",
        "\n",
        "for i in range(len(results)):\n",
        "  print('Predicted labels:')\n",
        "  for idx,val in enumerate(results[i]):\n",
        "    if val > 0.7:\n",
        "      print(label_encoder.classes_[idx])\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "tOnjcqoNXN5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the `CustomModelPrediction` class to make predictions and print the results"
      ],
      "metadata": {
        "id": "6CFUxFOoeGQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hope_count = len(data[data['label'] == 0])\n",
        "non_hope_count = len(data[data['label'] == 1])\n",
        "print(\"Hope count:\", hope_count)\n",
        "print(\"Non Hope count:\", non_hope_count )"
      ],
      "metadata": {
        "id": "88_sWJo4XPqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To count the occurrences of different labels in a DataFrame and print the counts"
      ],
      "metadata": {
        "id": "7sfMp0p5eT51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Under Sampling"
      ],
      "metadata": {
        "id": "5szMPSaNfPKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hope_count = len(data[data['label'] == 0])\n",
        "non_hope_count = len(data[data['label'] == 1])\n",
        "print(\"Hope count:\", hope_count)\n",
        "print(\"Non Hope count:\", non_hope_count )"
      ],
      "metadata": {
        "id": "cUCn7qyhfTgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To count the occurrences of different labels in a DataFrame and print the counts"
      ],
      "metadata": {
        "id": "dophVxTNfYHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newdata = data[data['label'] == 0].copy()\n",
        "newdata.reset_index(drop=True, inplace=True)\n",
        "newdata.head()"
      ],
      "metadata": {
        "id": "ieL3afyufnz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a new DataFrame containing only rows with a specific label ('label' = 0) and reset the index"
      ],
      "metadata": {
        "id": "0VcfPr8ufuuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hopeless = data[data['label']==1].copy()\n",
        "hopeless.reset_index(drop=True, inplace=True)\n",
        "hopeless.head()"
      ],
      "metadata": {
        "id": "CWwz9gzsgCIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a new DataFrame containing rows with a specific label ('label' = 1) and reset the index"
      ],
      "metadata": {
        "id": "z8qfMaHigJiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hopeless = hopeless.head(4000)\n",
        "hopeless.head()"
      ],
      "metadata": {
        "id": "rGxTQkDVgTXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To limit the number of rows in the DataFrame `hopeless` to 4000 and display the first few rows"
      ],
      "metadata": {
        "id": "6d_121PlgbeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(hopeless)"
      ],
      "metadata": {
        "id": "XHK2iflMgf9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Length of 'hopeless' dataframe is displayed"
      ],
      "metadata": {
        "id": "mTzNPfoqgkX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newset = pd.concat([newdata, hopeless], ignore_index=True)\n",
        "newset = shuffle(newset, random_state=22)\n",
        "newset.head()"
      ],
      "metadata": {
        "id": "WCei0ak0gk9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To concatenate two DataFrames, shuffle the combined DataFrame, and display the first few rows"
      ],
      "metadata": {
        "id": "QQkF10mYhGTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newset.to_csv('shortenedhope.csv', index=True)\n",
        "len(newset)"
      ],
      "metadata": {
        "id": "vGxFWtuMhLzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save the concatenated and shuffled DataFrame as a CSV file (shortenedhope.csv) and display its length\n"
      ],
      "metadata": {
        "id": "l0VEo5fRhP4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Newhope"
      ],
      "metadata": {
        "id": "HgOOVAHljzjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "metadata": {
        "id": "hmCsqqX_j1Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "snucK_ZKj67C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vW0JwylVj85v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/shortenedhope.csv')\n",
        "data = shuffle(data, random_state=22)\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "HwpxJ1RPj_qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [label for label in data['label'].values]\n",
        "print(labels, '\\n')"
      ],
      "metadata": {
        "id": "pUy7QryQkBkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_labels = []\n",
        "for label in labels:\n",
        "  if label == 1:\n",
        "    text_labels.append('non-hope')\n",
        "  elif label == 0:\n",
        "    text_labels.append('hope')\n",
        "print(text_labels)\n",
        "txt_labels = [[element] for element in text_labels]\n",
        "print(txt_labels)"
      ],
      "metadata": {
        "id": "nqtdtkrUkP1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = MultiLabelBinarizer()\n",
        "labels_encoded = label_encoder.fit_transform(txt_labels)\n",
        "num_labels = len(labels_encoded[0])\n",
        "print(data['text'].values[0])\n",
        "print(label_encoder.classes_)\n",
        "print(labels_encoded[0])"
      ],
      "metadata": {
        "id": "vGZdbCcokS1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size= int(len(data) * .8)\n",
        "print(f\"Train size: {train_size}\")\n",
        "print(f\"Test size: {len(data)-train_size}\")"
      ],
      "metadata": {
        "id": "dptpPTETkU0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = labels_encoded[:train_size]\n",
        "test_labels = labels_encoded[train_size:]"
      ],
      "metadata": {
        "id": "llQTGXuYkXdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile preprocess.py\n",
        "\n",
        "from tensorflow.keras.preprocessing import text\n",
        "\n",
        "class TextPreprocessor(object):\n",
        "  def __init__(self, vocab_size):\n",
        "    self._vocab_size = vocab_size\n",
        "    self._tokenizer = None\n",
        "\n",
        "  def create_tokenizer(self, text_list):\n",
        "    tokenizer = text.Tokenizer(num_words=self._vocab_size)\n",
        "    tokenizer.fit_on_texts(text_list)\n",
        "    self._tokenizer = tokenizer\n",
        "\n",
        "  def transform_text(self, text_list):\n",
        "    text_matrix = self._tokenizer.texts_to_matrix(text_list)\n",
        "    return text_matrix"
      ],
      "metadata": {
        "id": "9aPYtHD7kYWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocess import TextPreprocessor\n",
        "\n",
        "VOCAB_SIZE=50# This is a hyperparameter, try out different values for your dataset\n",
        "\n",
        "train_qs = data['text'].values[:train_size]\n",
        "test_qs = data['text'].values[train_size:]\n",
        "\n",
        "processor = TextPreprocessor(VOCAB_SIZE)\n",
        "processor._vocab_size=50\n",
        "processor.create_tokenizer(train_qs)\n",
        "\n",
        "body_train = processor.transform_text(train_qs)\n",
        "body_test = processor.transform_text(test_qs)"
      ],
      "metadata": {
        "id": "nAEB4r9okbPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(body_train[0]))\n",
        "print(body_train[0])"
      ],
      "metadata": {
        "id": "qPDVwCf9kecW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('./processor_state.pkl', 'wb') as f:\n",
        "  pickle.dump(processor, f)"
      ],
      "metadata": {
        "id": "bweiM7cpkgLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(vocab_size, num_tags):\n",
        "\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(50, input_shape=(VOCAB_SIZE,), activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(25, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(num_labels, activation='sigmoid'))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "5WvrlEQ2kh0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(VOCAB_SIZE, num_labels)\n",
        "model.summary()\n",
        "\n",
        "# Train and evaluate the model\n",
        "model.fit(body_train, train_labels, epochs=10, batch_size=128, validation_split=0.1)\n",
        "print('Eval loss/accuracy:{}'.format(\n",
        "  model.evaluate(body_test, test_labels, batch_size=128)))\n",
        "\n",
        "# Export the model to a file\n",
        "model.save('keras_saved_model.h5')"
      ],
      "metadata": {
        "id": "hV8y4vLmkkiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(body_test, test_labels, batch_size=128)"
      ],
      "metadata": {
        "id": "Eh3BntNqkml5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('keras_saved_model.h5')"
      ],
      "metadata": {
        "id": "qQu0koPHkqhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_prediction.py\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class CustomModelPrediction(object):\n",
        "\n",
        "  def __init__(self, model, processor):\n",
        "    self._model = model\n",
        "    self._processor = processor\n",
        "\n",
        "  def predict(self, instances, **kwargs):\n",
        "    preprocessed_data = self._processor.transform_text(instances)\n",
        "    predictions = self._model.predict(preprocessed_data)\n",
        "    return predictions.tolist()\n",
        "\n",
        "  @classmethod\n",
        "  def from_path(cls, model_dir):\n",
        "    import tensorflow.keras as keras\n",
        "    model = keras.models.load_model(\n",
        "      os.path.join(model_dir,'keras_saved_model.h5'))\n",
        "    with open(os.path.join(model_dir, 'processor_state.pkl'), 'rb') as f:\n",
        "      processor = pickle.load(f)\n",
        "\n",
        "    return cls(model, processor)\n"
      ],
      "metadata": {
        "id": "nQycpNVykr_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model_prediction import CustomModelPrediction\n",
        "\n",
        "classifier = CustomModelPrediction.from_path('.')\n",
        "results = classifier.predict(test_requests)\n",
        "print(results)\n",
        "\n",
        "for i in range(len(results)):\n",
        "  print('Predicted labels:')\n",
        "  for idx,val in enumerate(results[i]):\n",
        "    if val > 0.7:\n",
        "      print(label_encoder.classes_[idx])\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "3MCR9h3eks0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hope_count = len(data[data['label'] == 0])\n",
        "non_hope_count = len(data[data['label'] == 1])\n",
        "print(\"Hope count:\", hope_count)\n",
        "print(\"Non Hope count:\", non_hope_count )"
      ],
      "metadata": {
        "id": "Dbp7fzrlku2v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}